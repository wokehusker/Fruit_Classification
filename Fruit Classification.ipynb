{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apples, Plums, and Tomatoes! Oh My! - Image Classification Technology for Grocery Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the packages\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.image import imread\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import datetime\n",
    "\n",
    "#Set random state for reproducibility\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "###### The data cleaning stage consisted of 3 main steps. First, outside of the Jupyter Notebook, delete the subfolders labeled \"Apple D\", \"Apple E\", \"Apple F\", and \"Total Number of Apples\". This is done to fix any imbalance across the classes. Next, we must extract the individual raw image files from the data import. It's important to access individual filenames because it allows the images to be mapped to the correct class label in a Pandas DataFrame. This DataFrame is considered the \"source of truth\" for our model. Once we have the DataFrame established, we will set up a train, validate, test folder structure, with sub-directories based on class,  to form the foundation of our image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\acathcart\\\\Documents\\\\AI Academy\\\\Fruit Classification'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Locate the root directory we will be working with\n",
    "root_dir = os.getcwd()\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the path to the raw image data folders\n",
    "apple = root_dir+'\\\\apple'\n",
    "plum = root_dir+'\\\\plum'\n",
    "tomato = root_dir+'\\\\tomato'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the contents of the respective raw image data folders\n",
    "apple_dir = os.listdir(apple)\n",
    "plum_dir = os.listdir(plum)\n",
    "tomato_dir = os.listdir(tomato)\n",
    "\n",
    "#Create a list for the respective fruits to hold the underlying image filenames\n",
    "apple_fn = []\n",
    "for i in apple_dir:\n",
    "    apple_fn.append(i)\n",
    "    \n",
    "plum_fn = []\n",
    "for i in plum_dir:\n",
    "    plum_fn.append(i)\n",
    "\n",
    "tomato_fn = []\n",
    "for i in tomato_dir:\n",
    "    tomato_fn.append(i)\n",
    "\n",
    "#Combine filename lists into one comprehensive list\n",
    "all_fn = apple_fn + plum_fn + tomato_fn\n",
    "\n",
    "#Create a list for the respective fruits to hold the underlying correct classifications\n",
    "apple_class = []\n",
    "for i in apple_dir:\n",
    "    apple_class.append('apple')\n",
    "    \n",
    "plum_class = []\n",
    "for i in plum_dir:\n",
    "    plum_class.append('plum')\n",
    "    \n",
    "tomato_class = []\n",
    "for i in tomato_dir:\n",
    "    tomato_class.append('tomato')\n",
    "    \n",
    "#Combine classification lists into one comprehensive list    \n",
    "all_class = apple_class + plum_class + tomato_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Pandas DataFrame to hold image IDs and correct classification\n",
    "data_manual = pd.DataFrame()\n",
    "data_manual['id'] = all_fn\n",
    "data_manual['class'] = all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102red applee00901102.png</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103red applee00916103.png</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107red applee01001107.png</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108red applee01006108.png</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109red applee01021109.png</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Tamotoes00995.png</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Tamotoes00996.png</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>Tamotoes00997.png</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6901</th>\n",
       "      <td>Tamotoes00998.png</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6902</th>\n",
       "      <td>Tamotoes00999.png</td>\n",
       "      <td>tomato</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6903 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id   class\n",
       "0     102red applee00901102.png   apple\n",
       "1     103red applee00916103.png   apple\n",
       "2     107red applee01001107.png   apple\n",
       "3     108red applee01006108.png   apple\n",
       "4     109red applee01021109.png   apple\n",
       "...                         ...     ...\n",
       "6898          Tamotoes00995.png  tomato\n",
       "6899          Tamotoes00996.png  tomato\n",
       "6900          Tamotoes00997.png  tomato\n",
       "6901          Tamotoes00998.png  tomato\n",
       "6902          Tamotoes00999.png  tomato\n",
       "\n",
       "[6903 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quickly spot check the DataFrame to see that the images have been stored correctly\n",
    "data_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create directories for our train, validate, and test sets\n",
    "dir_names = ['train', 'validate', 'test']\n",
    "for group in dir_names:\n",
    "    new_dir = os.path.join(root_dir, group)\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "for fruit in ['apple', 'plum', 'tomato']:\n",
    "# Create sub_directories by fruit type\n",
    "    for group in dir_names:\n",
    "        new_dir = os.path.join(root_dir, group, fruit)\n",
    "        os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data Into Train, Validate, Test Sets\n",
    "#### We will randomly assign the raw images into train, validate, and test sets to safeguard the model from overfitting and give a more accurate evaluation of the model's performance. To do so, we will distribute the images along an 80:10:10 train : validate : test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Apple pictures.\n",
      "Split 2434 imgs into 1947 train, 243 val, and 244 test examples.\n"
     ]
    }
   ],
   "source": [
    "#Split the apple images into our train, validate, and test sets\n",
    "print('Moving {} pictures.'.format('Apple'))\n",
    "apple_df = data_manual[data_manual['class'] == 'apple']\n",
    "train_apple, validate_apple, test_apple = np.split(apple_df.sample(frac=1), [int(.8*len(apple_df)), int(.9*len(apple_df))])\n",
    "print('Split {} imgs into {} train, {} val, and {} test examples.'.format(len(apple_df),\n",
    "                                                                              len(train_apple),\n",
    "                                                                              len(validate_apple),\n",
    "                                                                              len(test_apple)))\n",
    "\n",
    "#Copy apple images to their correct directory & sub_directory   \n",
    "for i, temp in enumerate([train_apple]):\n",
    "    for row in train_apple.index:\n",
    "        filename = apple_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'apple' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'train' + '\\\\' + 'apple' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)\n",
    "            \n",
    "for i, temp in enumerate([validate_apple]):\n",
    "    for row in validate_apple.index:\n",
    "        filename = apple_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'apple' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'validate' + '\\\\' + 'apple' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)\n",
    "            \n",
    "for i, temp in enumerate([test_apple]):\n",
    "    for row in test_apple.index:\n",
    "        filename = apple_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'apple' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'test' + '\\\\' + 'apple' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Plum pictures.\n",
      "Split 2298 imgs into 1838 train, 230 val, and 230 test examples.\n"
     ]
    }
   ],
   "source": [
    "#Split the plum images into our train, validate, and test sets\n",
    "print('Moving {} pictures.'.format('Plum'))\n",
    "plum_df = data_manual[data_manual['class'] == 'plum']\n",
    "train_plum, validate_plum, test_plum = np.split(plum_df.sample(frac=1), [int(.8*len(plum_df)), int(.9*len(plum_df))])\n",
    "print('Split {} imgs into {} train, {} val, and {} test examples.'.format(len(plum_df),\n",
    "                                                                              len(train_plum),\n",
    "                                                                              len(validate_plum),\n",
    "                                                                              len(test_plum)))\n",
    "\n",
    "#Copy plum images to their correct directory & sub_directory   \n",
    "for i, temp in enumerate([train_plum]):\n",
    "    for row in train_plum.index:\n",
    "        filename = plum_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'plum' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'train' + '\\\\' + 'plum' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)\n",
    "            \n",
    "for i, temp in enumerate([validate_plum]):\n",
    "    for row in validate_plum.index:\n",
    "        filename = plum_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'plum' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'validate' + '\\\\' + 'plum' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)\n",
    "            \n",
    "for i, temp in enumerate([test_plum]):\n",
    "    for row in test_plum.index:\n",
    "        filename = plum_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'plum' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'test' + '\\\\' + 'plum' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Tomato pictures.\n",
      "Split 2298 imgs into 1736 train, 217 val, and 218 test examples.\n"
     ]
    }
   ],
   "source": [
    "#Split the tomato images into our train, validate, and test sets\n",
    "print('Moving {} pictures.'.format('Tomato'))\n",
    "tomato_df = data_manual[data_manual['class'] == 'tomato']\n",
    "train_tomato, validate_tomato, test_tomato = np.split(tomato_df.sample(frac=1), [int(.8*len(tomato_df)), int(.9*len(tomato_df))])\n",
    "print('Split {} imgs into {} train, {} val, and {} test examples.'.format(len(plum_df),\n",
    "                                                                              len(train_tomato),\n",
    "                                                                              len(validate_tomato),\n",
    "                                                                              len(test_tomato)))\n",
    "\n",
    "#Copy tomato images to their correct directory & sub_directory   \n",
    "for i, temp in enumerate([train_tomato]):\n",
    "    for row in train_tomato.index:\n",
    "        filename = tomato_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'tomato' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'train' + '\\\\' + 'tomato' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)\n",
    "            \n",
    "for i, temp in enumerate([validate_tomato]):\n",
    "    for row in validate_tomato.index:\n",
    "        filename = tomato_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'tomato' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'validate' + '\\\\' + 'tomato' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)\n",
    "            \n",
    "for i, temp in enumerate([test_tomato]):\n",
    "    for row in test_tomato.index:\n",
    "        filename = tomato_df['id'][row]\n",
    "        origin = os.path.join(root_dir + '\\\\' + 'tomato' + '\\\\' + filename)\n",
    "        destination = os.path.join(root_dir + '\\\\' + 'test' + '\\\\' + 'tomato' + '\\\\' + filename)\n",
    "        shutil.copy(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define more convinient way of calling path to train, validate, and test directories\n",
    "train_dir = '{}\\\\train\\\\'.format(root_dir)\n",
    "validate_dir = '{}\\\\validate\\\\'.format(root_dir)\n",
    "test_dir = '{}\\\\test\\\\'.format(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "#### We will be employing a pre-trained CNN (VGG19) to form the base of our model. This base allows our model to be significantly more robust in that it is now built upon a 19-layer deep neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,704,707\n",
      "Trainable params: 1,680,323\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize Base\n",
    "from keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet',\n",
    "                 include_top=False,\n",
    "                 input_shape=(240, 240, 3))\n",
    "\n",
    "# Define Model Architecture (AI Academy / Modules / \" \" Lab)\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#Freeze the layers\n",
    "cnn_base.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "#### This code chunk was inspired by the M4. Image Classification - Lab (https://github.com/learn-co-curriculum/dsc-image-classification-lab/tree/solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5521 images belonging to 3 classes.\n",
      "Found 690 images belonging to 3 classes.\n",
      "Found 692 images belonging to 3 classes.\n",
      "WARNING:tensorflow:From <ipython-input-23-37411653782f>:30: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 101s 10s/step - loss: 1.0117 - acc: 0.6020 - val_loss: 0.9442 - val_acc: 0.7440\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 102s 10s/step - loss: 0.8826 - acc: 0.7760 - val_loss: 0.8211 - val_acc: 0.8100\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 102s 10s/step - loss: 0.7704 - acc: 0.8140 - val_loss: 0.7098 - val_acc: 0.8360\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 104s 10s/step - loss: 0.6571 - acc: 0.8960 - val_loss: 0.6024 - val_acc: 0.9200\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 103s 10s/step - loss: 0.5705 - acc: 0.9160 - val_loss: 0.5176 - val_acc: 0.9080\n",
      "Training took 0:09:06.710486\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "# All images will be rescaled by 1/255\n",
    "datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "#Generate batches of normalized data\n",
    "train_generator = datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(240, 240),\n",
    "                                                    batch_size=50,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validate_generator = datagen.flow_from_directory(validate_dir, \n",
    "                                                        target_size=(240, 240), \n",
    "                                                        batch_size=50, \n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(240, 240),\n",
    "                                                  batch_size=692,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "#Compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = optimizers.RMSprop(lr=2e-5), #Learning rate: must be small to avoid overfitting\n",
    "              metrics=['acc'])\n",
    "\n",
    "#Fitting the Model\n",
    "history = model.fit_generator(train_generator,\n",
    "                             steps_per_epoch=10,\n",
    "                             epochs=5,\n",
    "                             validation_data=validate_generator,\n",
    "                             validation_steps=10)\n",
    "end = datetime.datetime.now()\n",
    "duration = end - start\n",
    "print('Training took {}'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0, 'plum': 1, 'tomato': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check batch index\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 692 images belonging to 3 classes.\n",
      "10/10 [==============================] - 59s 6s/step - loss: 0.5440 - acc: 0.8940\n",
      "Generated 500 predictions\n",
      "test acc: 0.8939999938011169\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model's performance using the test set\n",
    "test_generator = datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(240, 240),\n",
    "                                                  batch_size=50,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=10)\n",
    "y_hat_test = model.predict(test_generator, steps=10)\n",
    "print('Generated {} predictions'.format(len(y_hat_test)))\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
